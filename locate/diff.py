import json
import time
from collections import defaultdict
from pathlib import Path

import cv2
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
from ultralytics import YOLO

"""
diff.py æ¯”è¾ƒä¸åŒæ¨¡å‹ä¸åŒè¾“å…¥å°ºå¯¸çš„æ•ˆæœ
"""

# é…ç½®å‚æ•°
MODELS = [
    "./resource/weights/200e4b1440sz-s.pt",
    "./resource/weights/200e4b1440sz-n.pt",
    "./resource/weights/100e4b1440sz-s.pt",
    "./resource/weights/200e8b1440sz-n.pt",
]
IMAGE_SIZES = [640, 800, 1440]  # éœ€è¦æµ‹è¯•çš„è¾“å…¥å°ºå¯¸
IMAGE_DIR = "./resource/dataset/train/images/"
LABEL_DIR = "./resource/dataset/train/labels/"
OUTPUT_DIR = "../asset/locator/diff/"
METRICS_FILE = "../asset/locator/diff/model_metrics.json"


def yolo_to_xyxy(yolo_box, img_width, img_height):
    """
    å°†YOLOå½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡
    Args:
        yolo_box: [x_center, y_center, width, height] (å½’ä¸€åŒ–å€¼)
        img_width: å›¾åƒå®é™…å®½åº¦
        img_height: å›¾åƒå®é™…é«˜åº¦
    Returns:
        [x1, y1, x2, y2] åƒç´ åæ ‡
    """
    x_center, y_center, w, h = yolo_box
    x_center *= img_width
    y_center *= img_height
    w *= img_width
    h *= img_height

    x1 = max(0, x_center - w / 2)
    y1 = max(0, y_center - h / 2)
    x2 = min(img_width - 1, x_center + w / 2)
    y2 = min(img_height - 1, y_center + h / 2)

    return [x1, y1, x2, y2]


def get_image_size(img_path):
    """è·å–å›¾åƒå®é™…å°ºå¯¸"""
    img = cv2.imread(str(img_path))
    if img is None:
        raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {img_path}")
    return img.shape[1], img.shape[0]  # (width, heigh


def load_ground_truth():
    """åŠ è½½æ‰‹åŠ¨æ ‡æ³¨çš„çœŸå®æ ‡ç­¾"""
    gt = {}
    for label_file in Path(LABEL_DIR).glob("*.txt"):
        img_file = Path(IMAGE_DIR) / f"{label_file.stem}.jpg"
        if not img_file.exists():
            continue

        img_width, img_height = get_image_size(img_file)

        with open(label_file, 'r') as f:
            lines = [line.strip().split() for line in f.readlines()]

        gt[label_file.stem] = [
            {
                'class': int(parts[0]),
                'xyxy': yolo_to_xyxy(list(map(float, parts[1:5])), img_width, img_height),
                'conf': 1.0
            }
            for parts in lines if len(parts) >= 5
        ]

    return gt


def calculate_iou(pred, gt):
    """è®¡ç®—ä¸¤ä¸ªæ¡†çš„IoU"""
    # è®¡ç®—äº¤é›†åæ ‡
    x1 = max(pred[0], gt[0])
    y1 = max(pred[1], gt[1])
    x2 = min(pred[2], gt[2])
    y2 = min(pred[3], gt[3])
    # è®¡ç®—äº¤é›†é¢ç§¯
    inter_area = abs(x2 - x1) * abs(y2 - y1)
    # è®¡ç®—å¹¶é›†é¢ç§¯
    box1_area = (pred[2] - pred[0]) * (pred[3] - pred[1])
    box2_area = (gt[2] - gt[0]) * (gt[3] - gt[1])
    iou = inter_area / (box1_area + box2_area - inter_area)
    return iou


class AdvancedEvaluator:
    def __init__(self):
        self.gt_data = load_ground_truth()
        self.metrics = defaultdict(dict)

    def evaluate_model(self, model_path, imgsz):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹åœ¨ç‰¹å®šå°ºå¯¸ä¸‹çš„è¡¨ç°"""
        model = YOLO(model_path)
        model_name = f"{Path(model_path).stem}_{imgsz}"
        print(f"\nğŸ” æ­£åœ¨è¯„ä¼°: {model_name}")

        stats = {
            'total_images': 0,
            'correct_detections': 0,
            'false_positives': 0,
            'false_negatives': 0,
            'avg_iou': 0,
            'avg_conf': 0,
            'inference_time': 0
        }

        for img_path in Path(LABEL_DIR).glob("*.txt"):
            # è·å–å¯¹åº”å›¾ç‰‡è·¯å¾„
            img_file = Path(IMAGE_DIR) / f"{img_path.stem}.jpg"
            if not img_file.exists():
                continue

            # é¢„æµ‹å¹¶è®¡æ—¶
            start_time = time.time()
            results = model.predict(
                source=str(img_file),
                conf=0.4,
                iou=0.5,
                imgsz=imgsz,
                verbose=False
            )
            stats['inference_time'] += time.time() - start_time

            # å¤„ç†é¢„æµ‹ç»“æœ
            gt_boxes = self.gt_data.get(img_path.stem, [])
            pred_boxes = []
            for r in results:
                for box in r.boxes:
                    pred_boxes.append({
                        'class': int(box.cls),
                        'xyxy': box.xyxy.tolist()[0],
                        'conf': float(box.conf)
                    })

            # æ›´æ–°ç»Ÿè®¡æŒ‡æ ‡
            stats['total_images'] += 1
            matched = set()

            for gt in gt_boxes:
                best_iou = 0
                best_idx = -1

                for i, pred in enumerate(pred_boxes):
                    if pred['class'] == gt['class']:
                        iou = calculate_iou(pred['xyxy'], gt['xyxy'])
                        if iou > best_iou and i not in matched:
                            best_iou = iou
                            best_idx = i

                if best_iou > 0.7:
                    stats['correct_detections'] += 1
                    stats['avg_iou'] += best_iou
                    stats['avg_conf'] += pred_boxes[best_idx]['conf']
                    matched.add(best_idx)
                else:
                    stats['false_negatives'] += 1

            stats['false_positives'] += len(pred_boxes) - len(matched)

        # è®¡ç®—å¹³å‡å€¼
        if stats['correct_detections'] > 0:
            stats['avg_iou'] /= stats['correct_detections']
            stats['avg_conf'] /= stats['correct_detections']
        if stats['total_images'] > 0:
            stats['inference_time'] /= stats['total_images']

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        recall = stats['correct_detections'] / (stats['correct_detections'] + stats['false_negatives'] + 1e-6)
        precision = stats['correct_detections'] / (stats['correct_detections'] + stats['false_positives'] + 1e-6)
        stats['score'] = 0.4 * recall + 0.3 * precision + 0.2 * stats['avg_iou'] + 0.1 * (
                1 - stats['inference_time'] / 0.5)

        self.metrics[model_name] = stats
        return stats

    def run_evaluation(self):
        """æ‰§è¡Œå…¨é‡è¯„ä¼°"""
        for model_path in MODELS:
            for imgsz in IMAGE_SIZES:
                self.evaluate_model(model_path, imgsz)

        # ä¿å­˜ç»“æœ
        with open(METRICS_FILE, 'w') as f:
            json.dump(self.metrics, f, indent=2)

        # å¯è§†åŒ–ç»“æœ
        self.visualize_results()

        return self.metrics

    def visualize_results(self):
        """è‡ªé€‚åº”å¯è§†åŒ–æŠ¥å‘Šï¼ˆè‡ªåŠ¨è°ƒæ•´ç”»å¸ƒå°ºå¯¸ï¼‰"""
        # åŠ¨æ€è®¡ç®—ç”»å¸ƒå°ºå¯¸
        n_models = len(self.metrics)
        fig_width = max(18, n_models * 2)  # åŸºç¡€å®½åº¦ + æ¯æ¨¡å‹2è‹±å¯¸
        fig_height = max(15, fig_width * 0.8)

        # åˆ›å»ºç”»å¸ƒå’Œå­å›¾å¸ƒå±€
        fig = plt.figure(figsize=(fig_width, fig_height), constrained_layout=True)
        gs = fig.add_gridspec(2, 2, width_ratios=[1.2, 1], height_ratios=[1, 1])
        ax1 = fig.add_subplot(gs[0, 0])  # è´¨é‡æŒ‡æ ‡
        ax2 = fig.add_subplot(gs[0, 1])  # æ¨ç†é€Ÿåº¦
        ax3 = fig.add_subplot(gs[1, 0])  # é›·è¾¾å›¾
        ax4 = fig.add_subplot(gs[1, 1])  # å°ºå¯¸å½±å“

        plt.suptitle(f'å¤šæ¨¡å‹è¯„ä¼°æŠ¥å‘Šï¼ˆå…±{n_models}ç§é…ç½®ï¼‰', fontsize=16, y=1.05)

        # --- æ•°æ®å‡†å¤‡ ---
        model_names = []
        recalls, precisions, ious, speeds, scores = [], [], [], [], []

        for name, stats in self.metrics.items():
            model_names.append(name)
            recalls.append(stats['recall'] if 'recall' in stats else
                           stats['correct_detections'] / (
                                   stats['correct_detections'] + stats['false_negatives'] + 1e-6))
            precisions.append(stats['precision'] if 'precision' in stats else
                              stats['correct_detections'] / (
                                      stats['correct_detections'] + stats['false_positives'] + 1e-6))
            ious.append(stats['avg_iou'])
            speeds.append(1 / (stats['inference_time'] + 1e-6))
            scores.append(stats['score'])

        # --- å›¾1: è´¨é‡æŒ‡æ ‡å¯¹æ¯”ï¼ˆæ¨ªå‘æŸ±çŠ¶å›¾ï¼‰---
        y = np.arange(len(model_names))
        width = 0.25
        ax1.barh(y - width, recalls, width, label='å¬å›ç‡', color='#4C72B0')
        ax1.barh(y, precisions, width, label='ç²¾ç¡®ç‡', color='#55A868')
        ax1.barh(y + width, ious, width, label='å¹³å‡IoU', color='#C44E52')

        ax1.set_yticks(y, model_names, fontsize=9)
        ax1.set_title('(a) æ£€æµ‹è´¨é‡æŒ‡æ ‡å¯¹æ¯”', pad=10)
        ax1.legend(loc='lower right', bbox_to_anchor=(1, 0))
        ax1.grid(axis='x', linestyle='--', alpha=0.7)
        ax1.set_xlim(0, 1.1)

        # --- å›¾2: æ¨ç†é€Ÿåº¦ï¼ˆæ¨ªå‘æ¡å½¢å›¾ï¼‰---
        colors = plt.cm.viridis(np.linspace(0, 1, len(IMAGE_SIZES)))
        imgsz_to_color = {sz: colors[i] for i, sz in enumerate(IMAGE_SIZES)}

        bar_colors = [imgsz_to_color[int(name.split('_')[-1])] for name in model_names]
        bars = ax2.barh(model_names, speeds, color=bar_colors)

        # æ·»åŠ é€Ÿåº¦æ•°å€¼æ ‡ç­¾
        for bar in bars:
            width = bar.get_width()
            ax2.text(width + 5, bar.get_y() + bar.get_height() / 2,
                     f'{width:.1f}', ha='left', va='center', fontsize=8)

        ax2.set_title('(b) æ¨ç†é€Ÿåº¦ (FPS)', pad=10)
        ax2.grid(axis='x', linestyle='--', alpha=0.7)

        # æ·»åŠ å›¾ä¾‹è¯´æ˜å°ºå¯¸é¢œè‰²
        legend_elements = [plt.Rectangle((0, 0), 1, 1, color=color, label=f'{sz}px')
                           for sz, color in imgsz_to_color.items()]
        ax2.legend(handles=legend_elements, title='è¾“å…¥å°ºå¯¸', loc='lower right')

        # --- å›¾3: é›·è¾¾å›¾ï¼ˆç¼©å°æ ‡ç­¾å­—ä½“ï¼‰---
        categories = ['å¬å›ç‡', 'ç²¾ç¡®ç‡', 'IoU', 'é€Ÿåº¦', 'ç»¼åˆ']
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]

        for name, stats in self.metrics.items():
            values = [
                stats['correct_detections'] / (stats['correct_detections'] + stats['false_negatives'] + 1e-6),
                stats['correct_detections'] / (stats['correct_detections'] + stats['false_positives'] + 1e-6),
                stats['avg_iou'],
                min(1, 1 / (stats['inference_time'] + 1e-6)),
                stats['score']
            ]
            values += values[:1]
            ax3.plot(angles, values, 'o-', linewidth=1, markersize=3,
                     label=name.split('_')[0])  # åªæ˜¾ç¤ºæ¨¡å‹åŸºç¡€åç§°

        ax3.set_xticks(angles[:-1], categories, fontsize=8)
        ax3.set_title('(c) é›·è¾¾å›¾ç»¼åˆå¯¹æ¯”', pad=10)
        ax3.legend(bbox_to_anchor=(1.1, 1), fontsize=8)
        ax3.grid(True)

        # --- å›¾4: å°ºå¯¸å½±å“ï¼ˆåˆ†ç»„æŸ±çŠ¶å›¾ï¼‰---
        model_bases = sorted(set([name.split('_')[0] for name in model_names]))
        x = np.arange(len(model_bases))
        width = 0.25

        for i, imgsz in enumerate(IMAGE_SIZES):
            scores = []
            for model in model_bases:
                key = f"{model}_{imgsz}"
                scores.append(self.metrics[key]['score'] if key in self.metrics else 0)

            ax4.bar(x + i * width, scores, width, label=f'{imgsz}px',
                    color=imgsz_to_color[imgsz])

        ax4.set_xticks(x + width, model_bases, rotation=45, ha='right')
        ax4.set_title('(d) ä¸åŒå°ºå¯¸ä¸‹çš„è¯„åˆ†å¯¹æ¯”', pad=10)
        ax4.legend(title='è¾“å…¥å°ºå¯¸')
        ax4.grid(axis='y', linestyle='--', alpha=0.7)

        # è‡ªé€‚åº”è°ƒæ•´å¹¶ä¿å­˜
        plt.tight_layout()
        plt.savefig(
            f"{OUTPUT_DIR}/comparison.png",
            dpi=300,
            bbox_inches='tight',
            facecolor='white'
        )
        print(f"ğŸ“Š è‡ªé€‚åº”å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜è‡³: {OUTPUT_DIR}/comparison.png")


if __name__ == "__main__":
    # åˆ‡æ¢å­—ä½“
    try:
        matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # Windowsç³»ç»Ÿ
        matplotlib.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    except Exception as e:
        print(e)

    evaluator = AdvancedEvaluator()
    metrics = evaluator.run_evaluation()

    # æ‰“å°æœ€ä½³é…ç½®
    best_config = max(metrics.items(), key=lambda x: x[1]['score'])
    print(f"\nğŸ† æœ€ä½³é…ç½®: {best_config[0]}")
    print(f"   - ç»¼åˆè¯„åˆ†: {best_config[1]['score']:.3f}")
    print(
        f"   - å¬å›ç‡: {best_config[1]['correct_detections'] / (best_config[1]['correct_detections'] + best_config[1]['false_negatives']):.2%}")
    print(
        f"   - ç²¾ç¡®ç‡: {best_config[1]['correct_detections'] / (best_config[1]['correct_detections'] + best_config[1]['false_positives']):.2%}")
    print(f"   - å¹³å‡IoU: {best_config[1]['avg_iou']:.3f}")
    print(f"   - æ¨ç†æ—¶é—´: {best_config[1]['inference_time']:.3f}s/å¼ ")
